{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMf76DQqIXxjh/PFA7AIw2K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"mNEx6z4J0L97"}},{"cell_type":"markdown","source":["1. What is Simple Linear Regression?\n","> Simple Linear Regression is a statistical method used to model the relationship between a single independent variable (predictor, X) and a dependent variable (outcome, Y) by fitting a straight line to the observed data.\n","2. What are the key assumptions of Simple Linear Regression?\n","> 1. Linearity: The relationship between X and Y is linear.\n","> 2. Independence: The observations are independent of each other.\n","> 3. Homoscedasticity: The variance of the errors is constant for all values of X.\n","> 4. Normality: The errors (residuals) are normally distributed.\n","\n","3. What does the coefficient m represent in the equation Y=mX+c?\n","> The coefficient m is the slope of the regression line. It represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n","\n","4. What does the intercept c represent in the equation Y=mX+c?\n","> The intercept c is the y-intercept. It represents the predicted value of the dependent variable (Y) when the independent variable (X) is equal to zero.\n","\n","5. How do we calculate the slope m in Simple Linear Regression?\n",">\n","6. What is the purpose of the least squares method in Simple Linear Regression?\n","> The purpose of the least squares method is to find the best-fitting line by minimizing the sum of the squared differences (residuals) between the observed Y values and the predicted Y values from the model.\n","\n","7. How is the coefficient of determination (R) interpreted in Simple Linear Regression?\n","> The coefficient of determination (R\n","2\n"," ) represents the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X). For example, an R\n","2\n","  of 0.75 means that 75% of the variability in Y can be explained by the model.\n","\n","8. What is Multiple Linear Regression?\n","> Multiple Linear Regression is an extension of simple linear regression used to model the relationship between two or more independent variables (predictors) and a single dependent variable (outcome).\n","\n","9. What is the main difference between Simple and Multiple Linear Regression?\n","> The main difference is the number of independent variables. Simple linear regression has only one independent variable, while multiple linear regression has two or more.\n","\n","10. What are the key assumptions of Multiple Linear Regression?\n","> The assumptions are the same as for simple linear regression (Linearity, Independence, Homoscedasticity, Normality of residuals), with one additional key assumption: No Multicollinearity: The independent variables should not be highly correlated with each other.\n","\n","11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n","> Heteroscedasticity is the violation of the homoscedasticity assumption. It means the variance of the errors is not constant across all levels of the independent variables. It doesn't bias the coefficient estimates, but it makes the standard errors unreliable, leading to incorrect conclusions about the significance of the predictors.\n","\n","12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n","> 1. Remove one of the highly correlated variables.\n","> 2. Combine the correlated variables into a single predictor (e.g., using Principal Component Analysis - PCA).\n","> 3. Use regularization techniques like Ridge or Lasso regression, which are designed to handle multicollinearity.\n","\n","13. What are some common techniques for transforming categorical variables for use in regression models?\n","> 1. One-Hot Encoding: Creates new binary (0/1) columns for each category of the variable. This is the most common method.\n","> 2. Dummy Coding: Similar to one-hot encoding but creates k-1 columns for k categories to avoid multicollinearity.\n","> 3. Label Encoding: Assigns a unique integer to each category. This is generally not recommended for linear regression as it implies an ordinal relationship that may not exist.\n","\n","14. What is the role of interaction terms in Multiple Linear Regression?\n","> An interaction term is a variable created by multiplying two or more independent variables. It is included in the model to capture the effect where the influence of one independent variable on the dependent variable changes depending on the value of another independent variable.\n","\n","15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n","> 1. In Simple Linear Regression, the intercept is the predicted value of Y when the single X variable is 0.\n","> 2. In Multiple Linear Regression, the intercept is the predicted value of Y when all X variables in the model are 0.\n","\n","16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n","> The slope signifies the magnitude and direction of the relationship between an independent variable and the dependent variable. A larger absolute slope indicates a stronger effect. It directly affects predictions because it determines how much the predicted Y value will change for every one-unit increase in X.\n","\n","17. How does the intercept in a regression model provide context for the relationship between variables?\n","> The intercept provides a baseline or starting point for the prediction. It represents the predicted outcome when all predictors are zero, giving a foundational value from which the effects of the other variables are measured.\n","\n","18. What are the limitations of using R' as a sole measure of model performance?\n","> 1. R2 always increases when you add more predictors to the model, even if they are not useful. This can be misleading.\n","> 2. It does not indicate whether the coefficient estimates are biased.\n","> 3. A high R2 does not necessarily mean the model is a good fit for the data (e.g.,it could be overfitting). The Adjusted R2 is often preferred as it penalizes the addition of non-significant variables.\n","\n","19. How would you interpret a large standard error for a regression coefficient?\n","> A large standard error for a regression coefficient indicates that the estimate of that coefficient is imprecise and unreliable. There is a lot of uncertainty about the true value of the coefficient, and it may not be statistically significant.\n","\n","20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n","> Heteroscedasticity can be identified in a residual plot (residuals vs. predicted values) by a cone or fan shape, where the spread of the residuals either increases or decreases as the predicted values change. It's important to address it because it violates a key assumption of linear regression and can lead to incorrect inferences about the significance of your predictors.\n","\n","21. What does it mean if a Multiple Linear Regression model has a high R? but low adjusted R2?\n","> This situation suggests that the model likely includes many independent variables that do not significantly contribute to explaining the variance in the dependent variable. The high R2 is inflated by the sheer number of variables, while the low adjusted R2 reflects this by penalizing the model for the inclusion of these useless predictors.\n","\n","22. Why is it important to scale variables in Multiple Linear Regression?\n","> 1. It helps in interpreting coefficients when predictors are on different scales. The magnitude of a scaled coefficient directly relates to its importance.\n","> 2. It is essential for models that use regularization (like Ridge and Lasso) because they are sensitive to the scale of the inputs.\n","> 3. It can help gradient-based optimization algorithms converge faster.\n","\n","23. What is polynomial regression?\n","> Polynomial regression is a type of regression analysis where the relationship between the independent variable X and the dependent variable Y is modeled as an n-th degree polynomial. It is used to fit a non-linear relationship between variables.\n","\n","24. How does polynomial regression differ from linear regression?\n","> While linear regression fits a straight line (Y=mX+c), polynomial regression fits a curved line by adding polynomial terms (like X2,X3, etc.) to the equation. Despite modeling a non-linear relationship, it is still considered a type of linear model because the coefficients are linear.\n","\n","25. When is polynomial regression used?\n","> It's used when a scatter plot of the data suggests a curvilinear or non-linear relationship between the independent and dependent variables, which a simple straight line cannot capture effectively.\n","\n","26. What is the general equation for polynomial regression?\n","> The general equation for a polynomial regression of degree 'd'\n","\n","27. Can polynomial regression be applied to multiple variables?\n","> Yes, it can be applied to multiple variables. This involves creating polynomial features for each variable as well as interaction terms between them.\n","\n","28. What are the limitations of polynomial regression?\n","> The main limitation is the risk of overfitting. Using a very high-degree polynomial can make the model fit the training data perfectly but fail to generalize to new, unseen data. It can also be computationally expensive.\n","\n","29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n","> 1. Cross-validation: Test different degrees on validation sets to see which one generalizes best.\n","> 2. Adjusted R2 and information criteria (AIC, BIC): These metrics penalize model complexity, helping to find a balance between fit and complexity.\n","> 3. Visual inspection of the fitted curve to see if it makes logical sense.\n","\n","30. Why is visualization important in polynomial regression?\n","> Visualization is crucial to check for overfitting or underfitting. Plotting the fitted curve against the data points helps you see if the chosen polynomial degree captures the underlying trend without being excessively complex and \"wiggling\" to catch every single data point.\n","\n","31. How is polynomial regression implemented in Python?\n","> It is typically implemented using libraries like Scikit-learn. You would use the PolynomialFeatures class to create the polynomial terms from your original features and then fit a LinearRegression model to these new transformed features."],"metadata":{"id":"r3vpwv-Jw3a3"}},{"cell_type":"code","source":[],"metadata":{"id":"BJN90PuQ0Km-"},"execution_count":null,"outputs":[]}]}